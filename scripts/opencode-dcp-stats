#!/usr/bin/env python3
"""
Analyze Dynamic Context Pruning (DCP) tool impact on cache efficiency.
Tracks cache hit rates and context size changes before/after DCP tool invocations.

Usage: opencode-dcp-stats [--sessions N] [--min-messages M] [--json] [--verbose]
"""

import json
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Optional

# DCP tool names (across different plugin versions)
DCP_TOOLS = {
    "prune", "discard", "extract", "context_pruning",
    "squash", "compress", "consolidate", "distill"
}

# Anthropic pricing: cache read is ~10% of input cost
CACHE_READ_COST_PER_1K = 0.00030   # $0.30 per 1M tokens
INPUT_COST_PER_1K = 0.003          # $3.00 per 1M tokens


def get_session_messages(storage: Path, session_id: str) -> list[dict]:
    """Get all messages for a session, sorted by creation order."""
    message_dir = storage / "message" / session_id
    if not message_dir.exists():
        return []
    
    messages = []
    for msg_file in message_dir.glob("*.json"):
        try:
            msg = json.loads(msg_file.read_text())
            msg["_file"] = msg_file
            msg["_id"] = msg_file.stem
            messages.append(msg)
        except (json.JSONDecodeError, IOError):
            pass
    
    return sorted(messages, key=lambda m: m.get("_id", ""))


def get_message_parts(storage: Path, message_id: str) -> list[dict]:
    """Get all parts for a message, sorted by creation order."""
    parts_dir = storage / "part" / message_id
    if not parts_dir.exists():
        return []
    
    parts = []
    for part_file in parts_dir.glob("*.json"):
        try:
            part = json.loads(part_file.read_text())
            part["_file"] = part_file
            part["_id"] = part_file.stem
            parts.append(part)
        except (json.JSONDecodeError, IOError):
            pass
    
    return sorted(parts, key=lambda p: p.get("_id", ""))


def is_ignored_message(message: dict, parts: list[dict]) -> bool:
    """
    Check if a message should be ignored (DCP notification messages).
    Returns True if message has no parts OR all parts have ignored=true.
    Mirrors the isIgnoredUserMessage logic from the DCP plugin.
    """
    if not parts:
        return True
    
    # Check text parts for ignored flag
    text_parts = [p for p in parts if p.get("type") == "text"]
    if not text_parts:
        return False
    
    for part in text_parts:
        if not part.get("ignored", False):
            return False
    
    return True


def count_real_user_messages(storage: Path, session_id: str) -> int:
    """Count user messages that are not ignored (real user interactions)."""
    messages = get_session_messages(storage, session_id)
    count = 0
    
    for msg in messages:
        # Only count user role messages
        if msg.get("role") != "user":
            continue
        
        msg_id = msg.get("_id", "")
        parts = get_message_parts(storage, msg_id)
        
        if not is_ignored_message(msg, parts):
            count += 1
    
    return count


def extract_step_finish(parts: list[dict]) -> Optional[dict]:
    """Extract step-finish record from message parts."""
    for part in parts:
        if part.get("type") == "step-finish" and "tokens" in part:
            return part
    return None


def extract_dcp_tools(parts: list[dict]) -> list[dict]:
    """Extract all DCP tool calls from message parts."""
    dcp_calls = []
    for part in parts:
        if part.get("type") == "tool":
            tool_name = part.get("tool", "")
            if tool_name in DCP_TOOLS:
                dcp_calls.append({
                    "tool": tool_name,
                    "state": part.get("state", {}),
                    "part_id": part.get("_id", "")
                })
    return dcp_calls


def calc_cache_hit_rate(tokens: dict) -> float:
    """Calculate cache hit rate from token dict."""
    input_tokens = tokens.get("input", 0)
    cache = tokens.get("cache", {})
    cache_read = cache.get("read", 0)
    total_context = input_tokens + cache_read
    if total_context == 0:
        return 0.0
    return (cache_read / total_context) * 100


def analyze_session(storage: Path, session_id: str) -> dict:
    """Analyze DCP impact for a single session."""
    messages = get_session_messages(storage, session_id)
    
    result = {
        "session_id": session_id,
        "dcp_events": [],
        "total_dcp_calls": 0,
        "total_steps": 0,
        "by_tool": defaultdict(lambda: {
            "calls": 0,
            "hit_rate_before_sum": 0,
            "hit_rate_after_sum": 0,
            "context_before_sum": 0,
            "context_after_sum": 0,
            "input_before_sum": 0,
            "input_after_sum": 0,
            "cache_before_sum": 0,
            "cache_after_sum": 0,
            "events_with_data": 0
        }),
        # Track hit rates by distance from last DCP call
        "hit_rates_by_distance": defaultdict(list)
    }
    
    prev_step = None
    prev_dcp_tools = []
    steps_since_dcp = None  # None = no DCP yet, 0 = just had DCP, 1+ = steps after
    
    for i, msg in enumerate(messages):
        msg_id = msg.get("_id", "")
        parts = get_message_parts(storage, msg_id)
        
        step_finish = extract_step_finish(parts)
        dcp_tools = extract_dcp_tools(parts)
        
        if step_finish:
            result["total_steps"] += 1
            tokens = step_finish.get("tokens", {})
            curr_hit_rate = calc_cache_hit_rate(tokens)
            
            # Track hit rate by distance from last DCP call
            if steps_since_dcp is not None:
                result["hit_rates_by_distance"][steps_since_dcp].append(curr_hit_rate)
                steps_since_dcp += 1
            
            # If previous step had DCP tools, measure impact
            if prev_dcp_tools and prev_step is not None:
                prev_tokens = prev_step.get("tokens", {})
                
                prev_input = prev_tokens.get("input", 0)
                prev_cache = prev_tokens.get("cache", {}).get("read", 0)
                prev_context = prev_input + prev_cache
                prev_hit_rate = calc_cache_hit_rate(prev_tokens)
                
                curr_input = tokens.get("input", 0)
                curr_cache = tokens.get("cache", {}).get("read", 0)
                curr_context = curr_input + curr_cache
                curr_hit_rate = calc_cache_hit_rate(tokens)
                
                for dcp in prev_dcp_tools:
                    tool_name = dcp["tool"]
                    result["total_dcp_calls"] += 1
                    
                    event = {
                        "tool": tool_name,
                        "input_before": prev_input,
                        "input_after": curr_input,
                        "cache_before": prev_cache,
                        "cache_after": curr_cache,
                        "context_before": prev_context,
                        "context_after": curr_context,
                        "hit_rate_before": round(prev_hit_rate, 1),
                        "hit_rate_after": round(curr_hit_rate, 1),
                        "hit_rate_delta": round(curr_hit_rate - prev_hit_rate, 1),
                        "context_delta": curr_context - prev_context,
                        "message_id": msg_id
                    }
                    result["dcp_events"].append(event)
                    
                    # Aggregate stats
                    stats = result["by_tool"][tool_name]
                    stats["calls"] += 1
                    stats["hit_rate_before_sum"] += prev_hit_rate
                    stats["hit_rate_after_sum"] += curr_hit_rate
                    stats["context_before_sum"] += prev_context
                    stats["context_after_sum"] += curr_context
                    stats["input_before_sum"] += prev_input
                    stats["input_after_sum"] += curr_input
                    stats["cache_before_sum"] += prev_cache
                    stats["cache_after_sum"] += curr_cache
                    stats["events_with_data"] += 1
            
            prev_step = step_finish
            prev_dcp_tools = dcp_tools
            
            # Reset distance counter if this step had DCP tools
            if dcp_tools:
                steps_since_dcp = 0
    
    return result


def analyze_sessions(num_sessions: int = 20, min_messages: int = 5, output_json: bool = False, verbose: bool = False, session_id: str = None):
    """Analyze DCP impact across recent sessions."""
    storage = Path.home() / ".local/share/opencode/storage"
    message_dir = storage / "message"
    session_dir = storage / "session"
    
    if not message_dir.exists():
        print("Error: OpenCode storage not found at", storage)
        return
    
    # Get sessions to analyze
    if session_id:
        # Analyze specific session
        session_path = message_dir / session_id
        if not session_path.exists():
            print(f"Error: Session {session_id} not found")
            return
        sessions = [session_path]
    else:
        sessions = sorted(message_dir.iterdir(), key=lambda x: x.stat().st_mtime, reverse=True)[:num_sessions]
    
    all_results = []
    grand_totals = {
        "sessions_analyzed": 0,
        "sessions_with_dcp": 0,
        "sessions_skipped_short": 0,
        "total_dcp_calls": 0,
        "total_steps": 0,
        "min_messages_filter": min_messages,
        "by_tool": defaultdict(lambda: {
            "calls": 0,
            "hit_rate_before_sum": 0,
            "hit_rate_after_sum": 0,
            "context_before_sum": 0,
            "context_after_sum": 0,
            "input_before_sum": 0,
            "input_after_sum": 0,
            "cache_before_sum": 0,
            "cache_after_sum": 0,
            "events_with_data": 0
        }),
        "hit_rates_by_distance": defaultdict(list)
    }
    
    for session_path in sessions:
        session_id = session_path.name
        
        # Check minimum message count (excluding ignored messages)
        real_user_messages = count_real_user_messages(storage, session_id)
        if real_user_messages < min_messages:
            grand_totals["sessions_skipped_short"] += 1
            continue
        
        result = analyze_session(storage, session_id)
        result["user_messages"] = real_user_messages
        
        # Get session metadata
        title = "Unknown"
        for s_dir in session_dir.iterdir():
            s_file = s_dir / f"{session_id}.json"
            if s_file.exists():
                try:
                    sess = json.loads(s_file.read_text())
                    title = sess.get("title", "Untitled")[:50]
                except (json.JSONDecodeError, IOError):
                    pass
                break
        
        result["title"] = title
        
        if result["total_dcp_calls"] > 0:
            all_results.append(result)
            grand_totals["sessions_with_dcp"] += 1
        
        grand_totals["sessions_analyzed"] += 1
        grand_totals["total_dcp_calls"] += result["total_dcp_calls"]
        grand_totals["total_steps"] += result["total_steps"]
        
        for tool, stats in result["by_tool"].items():
            for key in stats:
                grand_totals["by_tool"][tool][key] += stats[key]
        
        # Aggregate hit rates by distance
        for dist, rates in result["hit_rates_by_distance"].items():
            grand_totals["hit_rates_by_distance"][dist].extend(rates)
    
    if output_json:
        output = {
            "sessions": all_results,
            "totals": dict(grand_totals),
            "generated_at": datetime.now().isoformat()
        }
        output["totals"]["by_tool"] = {k: dict(v) for k, v in grand_totals["by_tool"].items()}
        print(json.dumps(output, indent=2, default=str))
    else:
        print_summary(all_results, grand_totals, verbose)


def print_summary(results: list, totals: dict, verbose: bool = False):
    """Print human-readable summary."""
    print("=" * 110)
    print("DCP (Dynamic Context Pruning) Cache Impact Analysis")
    print("=" * 110)
    print()
    
    print("OVERVIEW")
    print("-" * 50)
    print(f"  Min user messages filter:   {totals['min_messages_filter']:>10,}")
    print(f"  Sessions scanned:           {totals['sessions_analyzed'] + totals['sessions_skipped_short']:>10,}")
    print(f"  Sessions skipped (short):   {totals['sessions_skipped_short']:>10,}")
    print(f"  Sessions analyzed:          {totals['sessions_analyzed']:>10,}")
    print(f"  Sessions with DCP:          {totals['sessions_with_dcp']:>10,}")
    print(f"  Total DCP tool calls:       {totals['total_dcp_calls']:>10,}")
    print(f"  Total steps:                {totals['total_steps']:>10,}")
    print()
    
    # Per-tool breakdown with cache hit rate and context changes
    if totals["by_tool"]:
        print("PER-TOOL BREAKDOWN")
        print("-" * 110)
        print(f"{'Tool':<18} {'Calls':>7} {'Avg Hit% Before':>16} {'Avg Hit% After':>15} {'Delta':>8} {'Avg Ctx Before':>15} {'Avg Ctx After':>14}")
        print("-" * 110)
        
        for tool, stats in sorted(totals["by_tool"].items(), key=lambda x: x[1]["calls"], reverse=True):
            calls = stats["calls"]
            if calls == 0:
                continue
            
            avg_hit_before = stats["hit_rate_before_sum"] / calls
            avg_hit_after = stats["hit_rate_after_sum"] / calls
            hit_delta = avg_hit_after - avg_hit_before
            avg_ctx_before = stats["context_before_sum"] / calls
            avg_ctx_after = stats["context_after_sum"] / calls
            
            delta_str = f"{hit_delta:+.1f}%"
            print(f"{tool:<18} {calls:>7,} {avg_hit_before:>15.1f}% {avg_hit_after:>14.1f}% {delta_str:>8} {avg_ctx_before:>14,.0f} {avg_ctx_after:>13,.0f}")
        print()
        
        # Cost analysis
        print("COST IMPACT ANALYSIS")
        print("-" * 80)
        total_input_before = sum(s["input_before_sum"] for s in totals["by_tool"].values())
        total_input_after = sum(s["input_after_sum"] for s in totals["by_tool"].values())
        total_cache_before = sum(s["cache_before_sum"] for s in totals["by_tool"].values())
        total_cache_after = sum(s["cache_after_sum"] for s in totals["by_tool"].values())
        
        # Tokens moved from cache to input = cache lost
        cache_preserved = total_cache_after
        cache_lost_to_input = max(0, total_cache_before - total_cache_after)
        
        if totals["total_dcp_calls"] > 0:
            # Average context reduction
            avg_ctx_before = (total_input_before + total_cache_before) / totals["total_dcp_calls"]
            avg_ctx_after = (total_input_after + total_cache_after) / totals["total_dcp_calls"]
            ctx_reduction = avg_ctx_before - avg_ctx_after
            
            print(f"  Total DCP events measured:     {totals['total_dcp_calls']:>12,}")
            print(f"  Avg context before DCP:        {avg_ctx_before:>12,.0f} tokens")
            print(f"  Avg context after DCP:         {avg_ctx_after:>12,.0f} tokens")
            print(f"  Avg context change:            {ctx_reduction:>+12,.0f} tokens")
            print()
            
            # Cache efficiency
            overall_hit_before = (total_cache_before / (total_input_before + total_cache_before) * 100) if (total_input_before + total_cache_before) > 0 else 0
            overall_hit_after = (total_cache_after / (total_input_after + total_cache_after) * 100) if (total_input_after + total_cache_after) > 0 else 0
            
            print(f"  Overall cache hit rate before: {overall_hit_before:>11.1f}%")
            print(f"  Overall cache hit rate after:  {overall_hit_after:>11.1f}%")
            print(f"  Hit rate change:               {overall_hit_after - overall_hit_before:>+11.1f}%")
        print()
    
    # Cache recovery analysis - hit rates by distance from last DCP call
    if totals["hit_rates_by_distance"]:
        print("CACHE RECOVERY ANALYSIS (Hit Rate by Steps Since Last DCP Call)")
        print("-" * 80)
        print(f"{'Steps After DCP':<18} {'Samples':>10} {'Avg Hit%':>12} {'Min':>10} {'Max':>10}")
        print("-" * 80)
        
        for dist in sorted(totals["hit_rates_by_distance"].keys())[:15]:
            rates = totals["hit_rates_by_distance"][dist]
            if rates:
                avg_rate = sum(rates) / len(rates)
                min_rate = min(rates)
                max_rate = max(rates)
                print(f"{dist:<18} {len(rates):>10,} {avg_rate:>11.1f}% {min_rate:>9.1f}% {max_rate:>9.1f}%")
        print()
        print("  (If cache fully recovers, later steps should approach 85%)")
        print()
    
    # Per-session breakdown (if verbose or few sessions)
    if verbose or len(results) <= 10:
        if results:
            print("PER-SESSION BREAKDOWN")
            print("-" * 110)
            print(f"{'Session':<25} {'Title':<30} {'User Msgs':>10} {'DCP Calls':>10} {'Avg Hit% Delta':>15}")
            print("-" * 110)
            
            for r in results:
                sid = r["session_id"][:24]
                title = r["title"][:29]
                user_msgs = r.get("user_messages", 0)
                avg_delta = 0
                if r["dcp_events"]:
                    avg_delta = sum(e["hit_rate_delta"] for e in r["dcp_events"]) / len(r["dcp_events"])
                delta_str = f"{avg_delta:+.1f}%"
                print(f"{sid:<25} {title:<30} {user_msgs:>10,} {r['total_dcp_calls']:>10,} {delta_str:>15}")
            print()
    
    # Individual DCP events (only in verbose mode)
    if verbose:
        print("INDIVIDUAL DCP EVENTS")
        print("-" * 110)
        for r in results:
            if r["dcp_events"]:
                print(f"\n  Session: {r['title'][:60]}")
                for event in r["dcp_events"][:15]:
                    ctx_delta = f"{event['context_delta']:+,}"
                    hit_delta = f"{event['hit_rate_delta']:+.1f}%"
                    print(f"    {event['tool']:<12} hit%: {event['hit_rate_before']:>5.1f} -> {event['hit_rate_after']:>5.1f} ({hit_delta:>7})  ctx: {event['context_before']:>8,} -> {event['context_after']:>8,} ({ctx_delta:>8})")
    
    print("=" * 110)


def main():
    parser = argparse.ArgumentParser(description="Analyze DCP tool cache impact")
    parser.add_argument("--sessions", "-n", type=int, default=20,
                        help="Number of recent sessions to scan (default: 20)")
    parser.add_argument("--session", "-s", type=str, default=None,
                        help="Analyze specific session ID")
    parser.add_argument("--min-messages", "-m", type=int, default=5,
                        help="Minimum real user messages required (default: 5)")
    parser.add_argument("--json", "-j", action="store_true",
                        help="Output as JSON")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="Show detailed per-event breakdown")
    args = parser.parse_args()
    
    analyze_sessions(
        num_sessions=args.sessions,
        min_messages=args.min_messages,
        output_json=args.json,
        verbose=args.verbose,
        session_id=args.session
    )


if __name__ == "__main__":
    main()
